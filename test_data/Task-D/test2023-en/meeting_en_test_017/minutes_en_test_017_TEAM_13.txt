we do nt have calen set calendar standard time eleven later at eleven
we will have i what about regular meetings on tuesdays what about next tuesday after it are cancelled
you can join like PERSON7 you online for example first second week
we should go through everything everybody comment everything can be discussed in middle of july
we could postpone it to thirty - one st thirty - one st of january like so with without financing finances must be spend
you set something parameter of neural machine translation training heat maps are different what
i have many PERSON7s comments it should be rewritten there are things are should not be should be should be changed it other things about the attentions cross - lingual attention start of attentions
i have section about highly about written attention heads paper from of PERSON14 et PERSON13 et al is finished there will be syntactic specific functions in attention heads will be written by PERSON3
it copy it to here inference of syntactic there should be section about interpretability it is inter interpretable not attentions is contents are it must be rewritten
i have nt seen papers concerned with embeddings in models its interesting to look into attention heads
three is less commentable four point three one four point three two need work four point four s m copied from elsewhere
i need to add more things for introduction PERSON9 by struc by structure
who is better is bad title of section title should be rewrittened contents what should be written
it anyway would be like three sections like only morphology syntax semantics it its better
we could copy like related work from paper is what
word2vec like main idea of is dot corresponds to one pair of results from one paper its hard to compare like dots between themselves bec different paper use different types of data think there 's many different things its possible to do comparison between two different underlying systems word embeddings recurrent neura neural
anything compare transformers in one paper there is like rnns there is
model had more data for pretraining then machine translation machine translation vs encoders were machine translation