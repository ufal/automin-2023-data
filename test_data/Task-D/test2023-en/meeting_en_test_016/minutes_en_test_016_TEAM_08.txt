And one of the early sessions is actually already being, or it could be transcribed by an annotator. And, this is something that 

(PERSON10) Look like, the very <unintelligible> solution, right? And the, on the, on the Monday seminars the static slides they even, they even hide the, the watch, so there is like clock. So that's, that's, that's the current thing that we have, but we still need to somehow organize the the layout to provide subtitles next to either subtitles or the paragraph you next to the slides. I'll be again inviting people to help, and I was curious, especially about [PERSON9], if he was like successful with his exams, and if he could now work on domain adaptation, so that we could feed in computational linguistics language model for, like [OTHER3] talk on computational linguistics, and if that would be, if we if we could plug this into the into the current setup. Then we need to include this word in the language model in all its forms. And, I dunno know, what is the the the key part of of your question, I would for simplicity assign all the words all these forms of the same word, the same probability. And so that's feminine known "reference", then I would use that list of all [LOCATION3] word forms to generate all the forms of this word, so "reference, "referenci", "referenci" this is actually boring, so this, this is very few. So, "reference", "růže", "růžích", and you would replace it with with all these words. <other_noise>

(PERSON3) <unintelligible>

Thank you, so I don't think that <unintelligible> ready and used for this Monday, but it will be very good if you synchronize with [PERSON9], and if you provided this to [PERSON9]

<other_noise>. That you should get in touch with [PERSON9], because [PERSON9] <unintelligible> some language model data related to my talk on Monday, so it would be great, if you to try to put this to Monday <unintelligible>. The back end is a server in C++, which is just reads the has gently reads the microphone signal, and then it sends it over web socket to the client, which is just a JavaScript piece of code that can be included in any webpage even on a remote machine. But as soon as we have the evaluation running, we can directly. (PERSON5) Yeah, measured <unintelligible> and there were differences. (PERSON3) Yeah, so, but you have the scripts to test the ASR. It's only for indication so far, so we need to to like to polish the the the indicators, and it's not, it's not critical in this development stage. (PERSON10) Like the many to many machine translation model and I used the cloud [OTHER5] to train it, at first, I just used a data set of 300 million open subtitles from various languages. It looks like the paraphrases are much better than the previous model. And I also received new data from [ORGANIZATION2], and I'm going to get it training today, and hopefully I will have some results, until until the end of the weekend. So as soon as you have the [PROJECT5] models, that correctly load on our GPU's, please email. (PERSON10) Yeah, well, there is one for <unintelligible> actually, that I could use it's called open neural network exchange format. I'm not sure if I'm not sure if if they support it, but it could be worth a try

(PERSON3) I don't think <unintelligible>, is it? Yeah, you are right yeah, yeah there is probably, there is probably not a high chance that the architecture [PROJECT5] is precisely the same. So the [OTHER3] [LOCATION3] model is on [OTHER1] serving, but I’m afraid that we are running [PROJECT5] models. (PERSON3) Yeah, that's the paraphrasing, but for [PROJECT2] purposes, simply the send all the various recordings that we have already transcribed, and we don't have translation for that, so we need to polish the data set, but just for. So the best corpus is actually the the best file in the <unintelligible> file which you have also translated into [OTHER2]. (PERSON3) Yeah, okay, great, that's running when we won't need it. (PERSON3) That's actually something which would be worthwhile if [PERSON11] could translate all these languages into [OTHER3], and [OTHER3] 

(PERSON10) Which language is it again? (PERSON5) So basically the whole incoming text will to keep on increasing entrance <unintelligible> full context and it will help the segmenter making more segments basically, so. (PERSON3) And another question, for what you are training on, because if you are training on correct text, and that's the case, then there is like a domain mismatch of looked again, and a few sentences that [PERSON2] was like saying in his [LOCATION3] presentation. (PERSON3) So that's a good question for [PERSON6], what data set has the correct transcripts which are as this fluent as the natural speech. (PERSON6) Okay, okay, yeah, that would be better I think

We can get in touch and definitely I can like, try to help you with that.