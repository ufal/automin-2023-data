So I'm curious, if [PERSON3] is here, we don't have [PERSON3]. <laugh>

(PERSON12) I explain that we don't have much progress with multi-source because we first need the data. And the all the corpora that we are collecting are document level if we can have them like that and they are as multilingual as they are. So that's something that [PERSON24] really, ah, should work on, after this baseline. So, I I really think that some experiments with multi-source text, ah, in [ORGANIZATION6] should be done immediately after -. Because the text are all already very, ah, very disambiguate. Ah, so it's the, ah, now it's the good point to mention the two biggest things. So, [PERSON10] is already hopefully in touch with, uh, everybody, uh, who is getting some of the [PERSON10]'s original tasks, uh. That the [PERSON6], here on the call, is not [PERSON10], right? And at the same time, we need to have the set up so that you can actually integrate and tested yourself. The full pipeline should be accessible to to him reasonably easily. And if this debugging can be done by the author of that component. But I think we <unintelligible> get on to bad he can simply download the files on the oval machines. So the profanity filtering should really be, ah, employed twice on each path. Ah, so, ah, I I don't think we need to fold proprietors spasm removal, because if we remove the the one from the ASR itself. So [PERSON3] this is another thing that you need to discuss with [PERSON10]. And, um, we should again get ready for for that with some domain adaptation. So [PERSON10] has probably started, but he should walk you through that thing. So, ah, on Monday we will hear the Italian English and yesterday, ah, we had a chance to hear the Japanese English and the ASR was really struggling with that. So that is, ah, that was the, ah, like news and and work that is being put on you, because [PERSON10] will be leaving. So, so, we still are totally relying on one person who is able to start the pipeline, and that is a bad situation. To, ah, like make sure that the whole set up is understandable and regularly tested by others in the team. So the the system needs users, and the more diverse users, the more different users, the better. So the language is were swaped there and some of the languages didn't contain any reasonable output. So that we are much more robusst even to individual persons not being available, right? Now you are getting this errors how to debugg what what section is exactly the hair issue with so first tribe with the ASR of then try with the <unintelligible> itself, try individual ASR <unintelligible> and things like that. (PERSON15) Is, is it better to run the multi lingual moral with fewer languages enabled? When we were testing a reef [PERSON18], when we were testing it on -. (PERSON15) So then the, there is actually no point in having [PERSON16] rainbow worker in the pipeline at the moment, right? But what I think is that if we could have like multiple word, multiple, multiple replicas of your same workers, and, each emitting, a different subset of languages. Like, I don't see that because it should be paralyzing when your -. (PERSON15) So I suggest that we do not use [PERSON16] tensor to tensor worker at all for the live sessions. Ah, as the fall back solution, we should have the [PROJECT3] models running as a worker on our side. (PERSON16) I think that I'm not either, but also, I am using the newest version of [PROJECT3] actually to train the shortening models. There were like chain of parts with which needed to be which needed to be fixed, and kind of unable to backtrack each each of them. (PERSON3) So [PERSON18] please send me on that e-mail as well. So let's let's use, ah, [PROJECT5] dash [ORGANIZATION5], also for the technical technical issues. So, right now, I am working on the shortening and extending models. So there they do have space, they only give it away only like after. (PERSON15) Because the practice is that no one ever cleans up after themselves. (PERSON16) And basically, ah, [PERSON19] would like to <unintelligible>. (PERSON15) So try, yeah, try that, because I'll have called at half past three with Dominic and and [PERSON24]. Because his master thesis is based on modern machine translation. So I'm for tommorow at two I'm trying to send a [ORGANIZATION2] invite. So, I'm now highlighting the experience, ah, from the [ORGANIZATION7] and as you want again. Because that specific is only lated to automating the ASR evaluation. So what essentially my script will be doing is taking an input of taking the index file as an input and it will generate the ASR from whatever model there is mentioned in the script. So, after that I'll have a call with [PERSON7] to discuss all the, oh, things that I put in the script, and then I think he can take it up from that. Yeah, idea is that once you evaluate exactly as flexible as you just describe it. Then, without running anything, you should just be able to look at the the stored outputs and the store scores, and it would immediately see where we are standing. Okay, so that that that was the the first immediate lesson and to do that that arose from the last week sessions. So that's is like, ah, a message for me but also for for [PERSON3]. So, the things that I spotted that really need attention are these. Ok, then, another thing that I spotted is this is in the Monday test document. If you find anyone else who would be ready to help with the immediate domain adaptation the data crunching please, ah, say so. So, my impression, ah, from the, ah, domain adaptation that [PERSON10] has been carefully doing for all the sessions, was that it was not really visible in the hybrid ASR. But if you could like dig into the toolkit, and debugg whether it is actually getting the worst into that. (PERSON3) Uh, okay, but for that, we will probably need of the transcribed. So this is like kind of backward looking, making sure that the old approach works the the new data well. (PERSON15) Yeah, so, if there would be anyone else, ah, would be curious about this, please let me know. (PERSON15) Yeah, I agree, except we don't have the the human capacity to do that. There was a debate whether languages, ah, are the African languages will be supported by some of the European project. And the answer from the project, ah, coordinator was that, well, we were discussing this a lot. And the translation was that, like we were disgusted by the idea of including African languages in our project. So even the word disgusting, which is not a bad word on his own, is very risky. Well, maybe we can use some sentiment analysis to to remove, um, sentences or or some a group of words with a negative sentiment. So uh it takes a lot of work, it will probably take a lot of work. There was actually a cable competition for this, where people had to make models to detect the tweets or like sentences which were harmful in some ways. But, ah, yeah, if it if it seems hateful in gender, we just remove it. So in in other words, the ASR should make a different hypothesis. Ah, I mean, we can just say in the profanity filter part, they can just look out for a hateful comments or or hateful sentence, and we can simply remove it. (PERSON15) I think he should stay with the non native English accent. If you, I would still I like to ask you, please read the experience of others, record your experience from the most recent sessions. I'm happy to see that actually we have covered by someone, to some extent, the- these topics that I've found. But there's probably more and, um, ah, yeah, we just need more people to do that. Okay, as well, other than like saying that my visa is a <unintelligible> probably be in Prague in January.