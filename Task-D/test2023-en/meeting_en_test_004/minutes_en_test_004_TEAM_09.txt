(PERSON13) [PERSON4] may or may not join us depending on <unintelligible>
(PERSON10) Yeah.(PERSON13) <unintelligible>
(PERSON10) Connect, actually connect.(PERSON7) [PERSON15] <unintelligible> [PERSON5] is on vacations.(PERSON10) Ok.
<unintelligible>a second.Yeah, so there is the chat window is being filled with failures of the ASR clients.I've, as I said, I've emailed [PERSON8], if [PERSON8] would be joining as well.These-This call is being recorded and there is no way to switch it off.So if you were not happy with that we would have to choose a different platform.And also, I don't think there is any way to switch off your webcams in the Alfa view, unless you simply like put a ducktape over ovet them.If you actually had a little video presentation, html client that I could test these streaming off slides with.And is there also some text only a client or text only version that was simply emit the subtitles to standard output.And I 'm asking because as a fallback, we could use like terminal window.And also, there is a chance of having the real video mixers that you know about.(PERSON3) [PERSON2] also propose me another option for the main stage presentation problem in order to present subtitles without the presentation platform in order to <unintelligible> 
But I have to investigate it little bit, maybe we can find different solution.I think that having a text client would be useful also for debugging.So if anything comes out that is also as a side effect that is also text client, it would be very useful.So you have already provided numerous <unintelligible> sample ways of examples how to run various clients.(PERSON13) Ok, ehm so I mentioned last time ehm the way this would work is that would be the [PROJECT1] server, [ORGANIZATION7] worker and in between a Python server which handles-<other_noise>Um, so last week I have written that and it seem to be working.So it now written a <unintelligible> in C, which can communicate with our server and send sends it <unintelligible> translations.I have tested that-<other_noise>And I've also add the example <unintelligible>, [ORGANIZATION7] worker client, ehm server, sorry.I don't know if I need to maybe write dummy ASR worker <unintelligible> test sentences or something like that.And I don't know if-[PERSON14], could you could you explain what exactly we would like to have for this type of testing?(PERSON7) And also here we work on this this week and it works with our Czech Czech machine translation, but there is still some bug on kick side, <unintelligible> only the first sentence.(PERSON14) The KT translation platform is very heavily <unintelligible> specific use case that we're using it in.It's also possible then some cases it's not confirming to the interface, so like sending the done message to early, which is why you are only getting the first message.Ehm some checks <unintelligible> that relate to partial sentences, for example, if one sentence in your chat testing data does not end in the period or a full stop.Then it will assume that is the first half of a partial sentence, and it will, it will output the sentence with dot dot dot at the end.And it will pend the next sentence because it thinks that them those two together make a full sentence.So that that's a lot of different checks in there.Ehm, so, it's-Personall, I 'm not sure exactly how will go, but fixings is because will require fixing the ASR, the segmentation and the MT from us, at the same time.(PERSON14) [PERSON5], [PERSON5] is the only one who's left.(PERSON7) Okay, so if every sentence on the input and with period, then <unintelligible> other sentences?(PERSON14) Though, worker round that you can do that works somewhat is ehm waiting between sending sentences.(PERSON7) Ok.
(PERSON14) As if the sentences were were spoken-<other_noise>
(PERSON3) Actually, this week we work together with [PERSON14] and [PERSON6], in order to find better workarounds.We also fix something I can-I fixed something in the mediator, in order to better manage second messages and also <unintelligible>.But I hope that we will find out a solution even if ehm, at the moment, the [ORGANIZATION5] worker are not meant to manage, the batch use case.Of course, at some point we have to manage it in the worker.So only for transcribing <unintelligible>, having <unintelligible> ASR output in order to train machine translation on this.(PERSON7) Yes, this is, I need the segmentation for- to obtain the train data.(PERSON7) For, for training MT <unintelligible> we would need it for for analysis of evaluation.So also [PERSON5] needs to test machine translation by looking at the output of the online ASR.Instead, I think it may be even better, if like [PERSON14] send <unintelligible> audio files to [PERSON5] and [PERSON5] did the ASR offline.Because we still would not get that true online segmentation since it is going to be changed at [ORGANIZATION5].Don't see any big benefit from fixing the batch processing at [ORGANIZATION5] through the mediator.(PERSON10) So, so maybe if [PERSON6], does that sound reasonable, so that [PERSON14] would send the audio files to [PERSON5] or to you, and you would do the ASR offline, and you would check the segmentation.I think you should send it to [PERSON5], because he's the one working with ASR.But I can say that our moduls are also not actually trained on the <unintelligible> segmentation.So I would think that that the segmentation is all prepared for offline ASR is reasonably close to what online ASR produces.So what is needed in order that, so that [PERSON5] can check that.I think the best way would be constantly running ASR worker, and [PERSON5] sending the audio file and making sure that the finger prints like connect the together.And [PERSON14] feat to this client, can you just repeat to me the exact set up for the testing of the [PROJECT1] worker connection.Ok.
(PERSON10) So what will you run on your command line, that's it.So what the, what will be the, the exact command, what will the command do?To in order to test the all integration <unintelligible>.(PERSON13) Ehm, yeah, I daily, I would have a ehm plain text file sentences resembling what's going to come from the ASR, and I would just feed adding to a client passes it through little mediator.Which then ehm <unintelligible> request of my [PROJECT1] worker, to do the translation.So such a client operate at the level of lines, so every line is a request, right?Yeah.Right, is that-
(PERSON13) Sorry, that's <unintelligible>I think [PERSON14] mentioned ehm, mentioned the getting stuck ehm.(PERSON10) Then, [PERSON6], what we need is to have lines of text that resemble the output of the ASR.And this is something that we would like to get from the audio recordings that you process, in offline mode, totally outside of of the mediator.I'll closely resemble the mediator with probably just a matter of configurations.So I I think we have offline ASR systems that are <unintelligible> and what's running in the mediator.And, I think the these ASR systems do their own segmentation, they don't use the same type of segmentation.The offline ASR does that in a different process.So there is a big risk that if we train on the segments from the ASR will get something very different from the on, from in the online mode.I see that as big risk, because it would be a different segmentation pipeline.(PERSON14) As far as I know our MT models for the mediator aren't trained on ASR, on actual ASR output at all.The ASR output does relatively close to resembles like TED talks and your <unintelligible> sort of spoken word empty datasets.We have a fine tuning <unintelligible> from lectures, but that also <unintelligible> the transcription was manually um improved just manually created in the first -
(PERSON10) Revised.So so it seems that the machine translation quality is not affected by mismatching segmentation between the training mode and the operation mode.I mean, what [PERSON6] is saying is they have have managed to get, or that actually tried with online ASR.I mean, what what what I understood is that [ORGANIZATION5] have already connected together online ASR and machine translation system trained independently on the current state of the art.You know, if we had a sample of online and offline, ten sentences, or I don't know, some small sample, just to compare.But there-
(PERSON10) So the easiest-
(PERSON13) <unintelligible>
(PERSON14) - everyday, I can send you some samples from the actual translator.(PERSON7) Yeah, well, I just say it's quite difficult to get lots of lots of online ASR, is that the problem, we can't really get the partial sentences of it.Is that what you are saying, that it is hard to get- It is hard to get 200000 sentences on online ASR in order to find tuning.(PERSON14) Well,-
(PERSON13) With transcript moved-
(PERSON10) Parallel data, that's it.(PERSON14) We have, we have some our fine tuning we have 15000 minds of lecture that were that were manually translated.We can get online ASR, we have 19 different lectures that are using online ASR.But-
(PERSON13) <unintelligible> ok.
(PERSON10) So I think our main concern is not the quality of the translation, but the mismatch of the segmentations.(PERSON14) So I can say this-
(PERSON13) It's terrible demo, try to make it better.(PERSON14) Updating and redoing the segmentation, <unintelligible> Is actually probably the next thing on our list of priorities, segmentation worker is the oldest component of that pipeline.Um, so what we are going to try to get <unintelligible> improve this quality of segmentation, to make it more <unintelligible> 
(PERSON10) Yeah, ok.And [PERSON6], please check that the line oriented output is as similar to the segmentation that we are going to receive from the segmentation <unintelligible>And [PERSON5] can in the meantime test the [PROJECT1] worker.So everything seems good, like for the baseline mode of operation, right?You agree or do you see any problems?Um, [PERSON14] is like it.I need to raise it a bit so, that the noise around-
(PERSON14) A quick sidewalk, because-
(PERSON10) Yes.(PERSON14) The chat is filling up with these messages but no ASR worker being available.I did just received a message from [PERSON5], apparently even though he is technically on vacation.Ehm, that he sees nothing wrong with the ASR workers, they are running, and they are, they're available <unintelligible>Maybe I, maybe I mispoken about which workers, I will check again.But we do not know what exactly the bug is that [PERSON5] will be fixing sooner or later.Okay, so thanks for the details on the [PROJECT1] worker, hopefully as we just said.So please just write it down that we, we have solved this differently by going to simply offline mode and will focus on the batch operation later on.Yes, so I think that's still this line that I've just highlighted in the [ORGANIZATION4] document, where [PERSON14] says," I also need either the [PROJECT1] worker or text client for CTM to TXT, or better both, to get the final hypothesis.(PERSON14) Oh yeah, the segmentation workers expect data of type unsegmented text and I'm not sure that a client exists, that send that sort of data.So I ask you to to check it, but since we just agreed that we are going to use your final we don't need it.(PERSON3) Of course next week we can perform again test all together in order to check both segmentation worker and other things together.(PERSON14) The segmentation worker shares a lot of the code with MT worker and therefore will probably have the same problem as the text client for the MT worker.But I don't have a precise time.(PERSON10) But still, I got, I'm sorry, I'm little bit lost.The CTM is the word level output of ASR.(PERSON10) But what we are expecting all are MT clients in- and the workers, including the [PROJECT1] one, expect already the text, so the segmented output, right?(PERSON10) In operation, even for the online mode, at the workshop in June, right?(PERSON7) Yes, they working on line mode, but not in <unintelligible> batch mode.They don't work, they don't work with CTM, but only only in sequence with first ASR and then segmentation.But but the ASR workers are not reliable <unintelligible> to to debug it or <unintelligible>
(PERSON10) To debug.Ok. 
(PERSON7) - is on ASR on <unintelligible>.So I understand that in order to test the machine translation, you always have to ship audio and you always have to rely on some ASR worker being available and that's problematic.And there is no immediate plan to to have such client, that would be able to digest CTM, send it and then use the segmentation worker in isolation, right?Is that correct?It should, there is no urge to fix this client, for the CTM, but at, but the condition log the testing of the machine translation.(PERSON10) So if so [PERSON5], if if you will be struggling with testing on by, testing your MT worker by shipping audio, then please, let us know earlier than next Friday.And let [ORGANIZATION5] know, that-
(PERSON13) Ok.
(PERSON10) Yeah, and hopefully [ORGANIZATION5] will find solution.And that's [ORGANIZATION5] ASR workers the bug.I'd like to have sort of a more directed bugging session for that.So a couple notable things that the problem seems to be that, um that after session is completed the client has send all of it stuff, recieved all of its output, stop sending anything.And it happens <unintelligible> mediator, which is the the old one in Adam and the [ORGANIZATION7] mediator, which is new in a Java.I don't think I've observed this with <unintelligible> text <unintelligible> very much.<other_noise>
(PERSON10) Aha, so [PERSON15], you're not here for the for second.(PERSON3) Yes, but actually I see that the ASR worker has start producing some text in the chat window.And by his summary if I try to re-That the ASR workers are <unintelligible><other_noise>And they are registered as non-ideal even when they have finish their jobs, and send there done messages to the mediator.And this happens with both the old mediator in <unintelligible> in Java, they happen for <unintelligible> as well for the for the ASR worker, right.And the only common thing that [PERSON6] sees is actually the worker, sorry, the AS- the sound client, the audio client.So, this is, [PERSON6] said that he would like like more hands on debugging session so that you would do this join it.(PERSON14) By the way, I'm looking at the ASR output in chat.They are removed by the segmentation workers, so in the in the MT input, these tags are not present.(PERSON14) Ehm, I'm really not sure what exactly we're we're seeing, because like I said, the segmentation worker should remove these.(PERSON10) But [PERSON15], you could know by looking at the mediator, you could know what fingerprint is [ORGANIZATION2] receiving.And that fingerprint would tell you, whether they are receiving the segmented output or the CTM already.I'd say <unintelligible>
(PERSON10) Because I think that from the users point of view, even if we are seeing pretty bad voice recognition here, because whatever sound conditions.(PERSON14) I 'm really not sure what it what it is recieving, because normally the the present our presentation platform doesn't handle unsegmented text.It only handles the text as it comes out of the segmentor-
(PERSON10) After-
(PERSON14) - will using our segmentor, than there shouldn't be any any text.I'm lot confused about <unintelligible> 
(PERSON10) So you have mentioned a number of things that you do like number of tricks that you do within your segmentation worker, like detecting the end of sentence, adding 3 dots and things like that.And this, all these tricks, can you, are there part of SLT [ORGANIZATION5]?Of the published scripts or not?So the SLT [ORGANIZATION5] is the sort of everything except the actual neural networks part that is doing the work.So SLT [ORGANIZATION5] is connected to the mediator, doing some preprocessing for MT, for example, like PPE, and that sort of thing.Is a sequence labeling-<other_noise>Labels each word with either opf insert coma or insert the full stop.(PERSON14) And-
(PERSON10) Capitalize, drop, or, yeah.(PERSON10) Yeah, because we, it's fairly reasonably described in the SLT [ORGANIZATION5].And I think that we could follow that, but when you were mentioning like adding three dots, for unfinished sentences, I didn't see anything like that in in the SLT [ORGANIZATION5].Then just the SLT [ORGANIZATION5] plus the train labourer model.So you think there is not not anything more, not any other component in the pipeline.Is just the labeling, the labourer, the trained <unintelligible> model and the the published scripts in the SLT [ORGANIZATION5], right?(PERSON3) I'm not able to check it right now, since log has been over written by something else.<unintelligible>, but just a reminder.(PERSON14) As far as I know our ASR worker connect, reconnect very quickly, the MT workers do as well, but it take some time.So this is important for [PERSON5] as well, and it's also important for us.We will be doing the ASR integrations, which is now like overseen by [PERSON14].We're not sure still if [PERSON14] will do it or someone else, but.(PERSON7) Ok.
(PERSON10) Yeah, ok.And the tricks, and [PERSON6] said that there is not anything extra beyond what's in SLT [ORGANIZATION5] plus the trained <unintelligible> model.If there are some other things, than the output of the ASR which will get from you offline, will be different from the output that will get from from this improved pipeline that you maybe using.So are we in in state, where we can do [PROJECT1] test next week, [PERSON5]?.So is your Github account [PERSON5]?So I'll-
(PERSON13) Yes.(PERSON7) - to Slack, so if you <unintelligible> need to discuss something online, than it's good <unintelligible>.So there will be session, session with <unintelligible> and [PERSON15] next week.[ORGANIZATION5] segmentation and MT, how is the fix, with the batch <unintelligible> sentence, so this is, this was, has been made irrelevant.Ignore batch mode until [PROJECT2], ok. Yeah, so this is for [ORGANIZATION2].We'll have four cabins with interpreters, students of interpretation, and will have one floor signal, which will be the original signal, will have 1 or 2 re-speakers, into Czech and from that will have interpretation into German.And we need to to decide online, which of these streams in concatenation with the ASR and the MT is the best set up.How will be killing the various pipelines and switching the the pipeline so that the presentation platform will jump on the different sources.And we can, we will have on the presentation platform an administration page where we can choose the main the selected stream for each languages.So that the main stream dies for some reason, we still have, we will still have the subtitles even if are not the the preferable ones.(PERSON10) Yes, so it now it seems that it will be all, yeah sorry, your office is just horrible, I could not work there for, it's really too loud.<laughing>So if we have four different pos- MT that translate to this many target languages that that you see here in the document.The Romanian, Polish, and then the Dutch, and Spanish.And we decide to switch the source of English, because the re-, how do we do this?So there is all the participants, and there is a Dutch participants and Spanish participants.And everybody is, someone is some Czech person is speaking in English, and his English is horrible.So there is this 2 re-speakers in their <unintelligible>, so is pronounciation, simply works better with the ASR.And what we need to do now is to make sure that that subtitles in Romanian are based on the machine translation all the target languages at once.Or we could do it one by one, but that would be like six times the the choice.(PERSON3) Well, this is not perform in the presentation platform, because the presentation platform is just um a presentation layer.(PERSON10) Ok. 
(PERSON3) This should be done, this should be decided in the selection of pipes in the mediator and should be done by the client at the moment.<unintelligible> who will want to see ASR outputs and find the component.Actually, it's a new component of the work project and we have to reason about it really carefully.The fallback solution that I see is that each of these source streams is assumed always pro all the sequence, that you mentioned, and we would be manually killing those that we do not like.The the the actual solution is to have a kind of combinatory explosion of all the possible match matching path.I I know it's not that <unintelligible>
(PERSON10) Yeah, yeah.(PERSON3) Yes, for example, the presentation platform will recieve for example, the German but I told based on English source or and based on for example <unintelligible> or English-
(PERSON10) English too.And if the output from one of the re-speaking cabin or the output from the floor is bad, this operator should kill the client that is unavailable, the machine translation will not be connected to that and further and the presentation platform will automatically jump to the other provided translation, right?(PERSON3) Yes, actually discussed in the last meeting.And the hardest part that I see is going back when there is speaker two again, becomes a good source.So that means that I need the ASR worker again, and it needs to connect to the MT worker again.(PERSON3) But actually, killing the client is useful only to ehm in computational power on servers which was the workers.(PERSON10) But I know, but the reason to kill the client is that I 'm not happy with the ASR output that I'm getting from that.If the presentation platform already, or if there was a man in the middle, I would use the man in the maddle, middle set up, to to disable this stream of input.(PERSON14) I think what [PERSON15] is proposing is that all audio inputs are translated into all target languages at all times.And that they matter of which one is shown to the to the audience is just thing in the presentation platform.(PERSON10) Yeah, so this is my prefer set up, as well.But I understood that there is no way to switch, which of the sources is the one to be presented while the whole system is running.So I would also prefer the presentation platform to have access to all the Spanish, and then choosing which of the Spanish is the best one.(PERSON10) Ok, so there will be someone monitoring the presentation platform, and we would know, by looking at these the lock files on the side that we are that we could like somehow hack together the monitoring in a separate window.And in the separate window will see that the manually select in the presentation platform stream number 44 is the one to show, because stream number 35 was big has become like of bad polity, right?(PERSON10) No, no, but how do I select different one?I will not see its output until I select it, but I can al-, I can select it, right?I can make a blind, within the presentation platform control, I can make a blind choice.So there is a high risk of me choosing something bad.(PERSON10) OK so then the indeed, we, will not have men in the middle, but will have a man watching logs from the the ASR workers and logs from the MT systems.And the same man doing the choice in the presentation platform, which is blind that choice, because I missread the IDs.Then I will see immediately that the stream that I've selected is wrong one and I'll choose a different one.(PERSON3) Ok, is the man who's performing the monitoring is able to access to the ASR log and to the machine translation log, yes.Yeah, can there be more people monitoring the same presentation platform at the same time, so that one would be checking the Spanish outputs, and one would be checking the Dutch outputs, and one re-checking the German ones, and they would like simultaneously make their decisions.Ehm, is the same logic as an <unintelligible> user may see the English final subtitle or French one, that's it.(PERSON10) So it's just a multitude of those.So the client will connect to a particular stream of publishing subtitles and this is to both for the the one who's configuring the system.And the normal user is selecting which language he wants to see and this super user, the monitor of the presentation platform is choosing for all the followers of of Spanish, which Spanish source should they get, right?(PERSON3) Yes, actually I hope that this is not a something that we have to choose so many times during the the conference.One of the persons will be responsible for making the the Polish output using the best ASR, another would be responsible for making Spanish output using the best ASR, and they all would look at the ASR and they would like indicate to each other, which of the ASR is is the best at the moment.(PERSON3) Sorry [PERSON10], just a technical question, our integration has ask me to report you, they are pretty care by the fact that 4000 people connects on the same WiFi network.I'd saying upfront, we also don't believe that 400 can can receive the signal our WiFi reasonably well.(PERSON10) Yeah, yeah, but still, you are the integrator so somehow we need to to come up with the solution.But we could ask [ORGANIZATION1] to organize the people by the language and to put like screens in front of group that that wants to read Polish subtitles.(PERSON3) Ok.
(PERSON10) Well, I don-We could do other things, we could, there is I know of a company, because [PERSON1] mentioned that.Like a friend of hers has various tools to support subtitling and also transcribing of speech, manual transcribing.So is like a little little bar of letters and you can send the this the text to that.So in that way, we would have a multiple of those and that would that would work.Yeah, so let's leave this for the for the [PROJECT3] call.I hope that also [PERSON2] which is a more expert technician will comes up with a brilliant solution, about this part.And let's talk again next Friday and in the meantime, hopefully you'll report to me that some of the bugs were resolve.And that all everything works, and that we should ship our [PROJECT1] models to to [PERSON5] and then- or we should test the [PERSON5] workers with our [PROJECT1] model.(PERSON7) [PERSON5], I'm sending you an e-mail right now.(PERSON10) Ok, yeah, thank you, bye.