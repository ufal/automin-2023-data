So the news from me are that, we had session like live workshop dry run at the [LOCATION2] on Wednesday and all the presentations went kind of well for the people who could understand [OTHER3], because that was run in [OTHER3], and people who had to rely on our subtitling were totally lost.<laugh>

So, the, the head of the particle division of the [ORGANIZATION1] was at first very angry like not, not allowed to, to everybody, but I was talking to him for more than five minutes, maybe ten minutes in the corridor.And we would like, so it has to be something that we can ASR, and [OTHER6] is the, the best next option, and we will try to rely on the live generated translations into [OTHER3], for example, and we will see how bad the translations will be.So if anybody can join us, if anybody does not speak [OTHER6], and would you have an idea what we would like to see, what what what would be an interesting [OTHER6] talk, please email us, get in touch and we'll do it as as, like joined watching.So that's, that's one piece of news another bit is that the critically required, and as part of the usability for the users, the critical required the video and subtitles or the slides, and subtitles to be on the same screen.And, this is something that we also know about, [PERSON10] has created something which is perhaps better than streaming of video, because the standard video players are designed,

<other_noise> 

to play in a continues way.(PERSON10) Um, I was saying that it looks like very like <unintelligible> solution, like you know, hitting with hitting it with the hammer

(PERSON3) Yes, but it's, I don't think it's too bad solution, because it's intended for slides, there is usually no, it's, it's not good for videos obviously, there is no way this would work for videos, but it's good for static slides, and the setup that we have on Monday seminars all the time, and, um.So that's, that's, that's the current thing that we have, but we still need to somehow organize the the layout to provide subtitles next to either subtitles or the paragraph you next to the slides.So we have one more person who just came, and that's [PERSON7], he was um, visiting master students so to say, for semester here, and how, he's now going to leave back for [LOCATION2] or, yeah [LOCATION2] <unintelligible>.(PERSON6) Okay, <unintelligible>, living in [LOCATION2] whole time.So, [PERSON7] agreed to join us for the surge, depending on his availability he's or his some some paper to finish that, like project to finish, to make into paper, and then also help us in whatever is needed, so that the the surge documents.So that was his since, I really have limited time, the last bit is that on Monday at two I'm giving a talk on the Monday seminar.I'll be again inviting people to help, and I was curious, especially about [PERSON9], if he was like successful with his exams, and if he could now work on domain adaptation, so that we could feed in computational linguistics language model for, like [OTHER3] talk on computational linguistics, and if that would be, if we if we could plug this into the into the current setup.Again, I'm reminding you of, of the sheet, please enter your achievements, so as soon as you've accomplished something, add to the line there, otherwise we'll forget who did what, and that's too bad for the bonuses.(PERSON9) Alright, so I managed to-  would almost <unintelligible>.<other_noise>

I, I've sent an email to the <unintelligible>, about the, what I found, so if anyone could look into it, and say something, it would be great.(PERSON3) Ah, I'm <unintelligible>, I'll double check.So the idea is that we need to provide all the forms, so as soon as a word is found in, because your processing all the related materials, if the related materials, if we're focusing on [LOCATION3], if the related materials contain one word.So I would like to, a replacement word, well actually not, I would choose a replacement word and use the corresponding forms of the replacement word.And so that's feminine known "reference", then I would use that list of all [LOCATION3] word forms to generate all the forms of this word, so "reference, "referenci", "referenci" this is actually boring, so this, this is very few.And I would the pronunciation is something that you easily generate, and the replacement word should be some random feminine noun, again, the similar, ideally with similar declaration pattern.So, "reference", "růže", "růžích", and you would replace it with with all these words.<other_noise>

(PERSON3) Okay, yeah, so yeah, I'll have a look at that.<cough>

So, is there any chance that you would have some files ready for the Monday talk, or not at all if not that's not a problem, either, but there is all of the semesters are starting, so every Monday there will be some more technical talk in [OTHER3], mostly in [OTHER3] on <unintelligible>

<other_noise>

meeting.(PERSON3) Yeah, so please get in touch with [PERSON5], I dunno where is [PERSON5] today, I forgot to, uh, I haven't seen him in the office.(PERSON6) Okay, so I have <unintelligible> mostly the work and the <unintelligible>, I have it for [LOCATION3], but <unintelligible> too difficult to modify for [OTHER3], so, um, I <unintelligible>, if I received some domain input sentences, then I could search the database, and return a list of a singular sentence states, and some large focus, and it should also work on word level, so if I give it uh, <unintelligible>, then it will find a sentences that are like <unintelligible>, so it maybe, because I will be in the mountains on the weekend, but I will be back in like Sunday night, so <unintelligible> for the Monday, and then I will be able to provide something, but probably either like Monday morning or Sunday late night, <unintelligible>.<other_noise>

(PERSON3) <unintelligible>

Thank you, so I don't think that <unintelligible> ready and used for this Monday, but it will be very good if you synchronize with [PERSON9], and if you provided this to [PERSON9]

<other_noise>.Then if you could search before, search through a huge corpus of the particular language, [LOCATION3] or [OTHER3] for similar sentences and see how much much this corpus expansion can provide us with related other texts, other sentences.(PERSON6) Okay, okay, so it will be best to if we synchronize with [PERSON9], you can try <unintelligible>, I'm not sure how how, it can be like I just <unintelligible>.<other_noise>

(PERSON3) It depends on the domain, so if you are searching for <unintelligible>, the chances <unintelligible> zero.That you should get in touch with [PERSON9], because [PERSON9] <unintelligible> some language model data related to my talk on Monday, so it would be great, if you to try to put this to Monday <unintelligible>.And the first thing is the visualization, how was it called, real time audio visualization tool on the old realization tool, which is 

<cough>

which basically I, I implemented I implemented a back end and the front end.The back end is a server in C++, which is just reads the has gently reads the microphone signal, and then it sends it over web socket to the client, which is just a JavaScript piece of code that can be included in any webpage even on a remote machine.I also made it so that it colors in different colors, depending on the volume, though, the the last problem is that I'm not sure how to stand those thresholds, and I wrote into email to [PERSON5] that that I could do in in like three ways either I could just like.Or I could just choose a recording from the from the minuting corpus, and I could perhaps, play the recording for myself and see how loud it is, and then see whether the.(PERSON3) ASR <unintelligible>

(PERSON10) Threshold.So if you if you have the the pick, any of their recordings and played at various loudness levels, <unintelligible>

<other_noise>

the level, and and as indicated or is calculated by your application, and then sat the threshold, so that they work for this, and if you are running any problem problems just set it to some random value.So maybe if the output could be a little bit more verbose so, that some number, would appear next to the next to the the image as well, so that when [PERSON5] is observing the session, and he notices that something is too quiet, and or too loud, he could mark down the number, so that he doesn't have to take a screenshot and then.(PERSON10) Like the many to many machine translation model and I used the cloud [OTHER5] to train it, at first, I just used a data set of 300 million open subtitles from various languages.And I also received new data from [ORGANIZATION2], and I'm going to get it training today, and hopefully I will have some results, until until the end of the weekend.I'm not sure if I'm not sure if if they support it, but it could be worth a try

(PERSON3) I don't think <unintelligible>, is it?(PERSON3) No it's not [PROJECT4]

It's like, neural network export format, but the critical thing is is that you have to know the architecture of the network.(PERSON3) Yeah, so that's great news for paraphrasing, like projects, but I don't think there is any way in which we could benefit from that for [PROJECT2], because 

(PERSON10) Does it have to be in [PROJECT5]?So the [OTHER3] [LOCATION3] model is on [OTHER1] serving, but I’m afraid that we are running [PROJECT5] models.So we are running [PROJECT5] models, which are trained in a similar way, but still well, [PROJECT5], [PROJECT5] only and not [OTHER1] serving.(PERSON3) Yeah, and see if we, so that's a thing for [PERSON5] again, as we are growing, we still have not collect, obtained any single number from our models, in the in as they are in this setup, but we should get to that in in in the two weeks of February at the latest, so within these two weeks.(PERSON10) If you could just send me, just some data set or or a link, or or just if it's new stats what it is.(PERSON10) But it is, but it is trained like in [OTHER3] centric way, and and when I evaluated it on [OTHER3], it has much better performance than in the other languages obviously, because it sees [OTHER3] all the time.(PERSON3) Yeah, that's the paraphrasing, but for [PROJECT2] purposes, simply the send all the various recordings that we have already transcribed, and we don't have translation for that, so we need to polish the data set, but just for.So the best corpus is actually the the best file in the <unintelligible> file which you have also translated into [OTHER2].Perfect, so that was [PERSON11], and then there is nobody else on the remote call, so maybe [PERSON8] 

(PERSON10) This week i worked on data collection, is basically four five languages, <unintelligible> five languages so almost all the languages have finished, but some <unintelligible> remaining.(PERSON3) That's actually something which would be worthwhile if [PERSON11] could translate all these languages into [OTHER3], and [OTHER3] 

(PERSON10) Which language is it again?(PERSON3) So, it's the it's the big ones, [OTHER3], [OTHER6],[OTHER2], [OTHER4] and.(PERSON10) Yeah, so I would just make a model where I would throw away the lower source languages, and I would just use the main ones.(PERSON3) So [PERSON8] is collecting monolingual data, and that means that we need to translate this to create synthetic parallel data.(PERSON3) Are better than the old [ORGANIZATION2] one, I'm not believe that as well, then this is the good model to do the offline translations of all these large data.(PERSON10) No no, right now it isn't modeling <unintelligible> some documents<unintelligible> available on parallel

(PERSON3) But we've we've agreed that the parallel data extraction will be done by [ORGANIZATION2].(PERSON3) So just like leave those parallel files for them and let's use this single, the single side of those for the for the back translation.So please send the path to all the source files that you have to [PERSON11] and [PERSON11] will apply the models, and we will get the first follow synthetic, very much in domain corpus, across these six main languages, and then redo this also with all the other 30 something languages.(PERSON6) Just let you know, I'm just going back next week, I work part time, so that's the reason I just had limited time for this project, and most of my tasks are <unintelligible> translations from [OTHER3] to [OTHER2] to feed anything <unintelligible>.(PERSON5) Hi everyone, so I'll just make it quick <unintelligible> how much this week, so I spend most of the time like preparing for the dry run and [ORGANIZATION1] event which was yesterday.The, and I'm not able to run the script basically to deploy their worker on the mediator, and the installation was succesful, so this is like few our tasks, which I concerned <unintelligible>.(PERSON5) <unintelligible>

But how her like I observed that when you give the complete ASR output like <unintelligible> at once segmenter, it gives a very impressive segments in the text.But when it isn't ASR <unintelligible> text keeps on coming, the segments are less, very very less.(PERSON3) So is there a way in which you could train it so that it is more similar to the ASR output.(PERSON5) So that's why how [PERSON12] helped me, <unintelligible> buffer, I will be taking all the text and so that we like, all incoming text will.(PERSON5) So basically the whole incoming text will to keep on increasing entrance <unintelligible> full context and it will help the segmenter making more segments basically, so.(PERSON3) And another question, for what you are training on, because if you are training on correct text, and that's the case, then there is like a domain mismatch of looked again, and a few sentences that [PERSON2] was like saying in his [LOCATION3] presentation.And naturally, he is a repeating phrases it's jumping out of the sentence, and then and then coming back to the sentence and this is some, this is a second output which we cannot handle.(PERSON5) I was reading newspaper, and I came across that they trained <unintelligible> Estonian data set, and they also used some in domain Estonian data and all select auto domain.(PERSON5) So they mixed both of them, <unintelligible> train the, so I am probably looking for this contact, maybe [PERSON6] to get me some [LOCATION3] in domain data.The the the fact that the [PERSON6] sentences are on the same topic or whether the sentences are as the influent as in normal speech.(PERSON5) No, I mean data <unintelligible> [LOCATION3] segment <unintelligible>, domain data.(PERSON10) I have a quick idea, could we perhaps make just something like, Wikipedia page, or something like that with at least all possible data sets or all possible like you know purposes, so that so that when someone like has a new one, we can always just add it there, so it can be used by everyone else.So just please hold on, and we are kind of what you want

(PERSON10) Yeah, okay, okay, because like I, I also have some processed data that I collected, and so, so I could send it somewhere and so on.(PERSON10) Terrible, <unintelligible> 

(PERSON3) Other data and know about them, work for [PERSON2].So, thank you very much for coming, next I'll email everybody again, at the latest on Wednesday, when are we meeting next week, and there is Doodle <unintelligible>, so please keep that Doodle <unintelligible>, like up to date because the Doodle <unintelligible> applies to regular weekly time, and we will find the date, which time which suits most of you.